{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eosC0fERt6DY"
   },
   "source": [
    "# Gradient Episodic Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2uLpHMYD4iT-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import quadprog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0NTE-5OtXG-"
   },
   "source": [
    "### Define P-MNIST data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlE3KQNSstue"
   },
   "outputs": [],
   "source": [
    "class PermutedMNISTDataLoader(torchvision.datasets.MNIST):\n",
    "    def __init__(self, source='data/mnist_data', train = True, shuffle_seed = None):\n",
    "        super(PermutedMNISTDataLoader, self).__init__(source, train, download=True)\n",
    "        \n",
    "        self.train = train\n",
    "        self.num_data = 0\n",
    "        \n",
    "        if self.train:\n",
    "            self.permuted_train_data = torch.stack(\n",
    "                [img.type(dtype=torch.float32).view(-1)[shuffle_seed] / 255.0\n",
    "                    for img in self.train_data])\n",
    "            self.num_data = self.permuted_train_data.shape[0]\n",
    "            \n",
    "        else:\n",
    "            self.permuted_test_data = torch.stack(\n",
    "                [img.type(dtype=torch.float32).view(-1)[shuffle_seed] / 255.0\n",
    "                    for img in self.test_data])\n",
    "            self.num_data = self.permuted_test_data.shape[0]\n",
    "            \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.train:\n",
    "            input, label = self.permuted_train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            input, label = self.permuted_test_data[index], self.test_labels[index]\n",
    "        \n",
    "        return input, label\n",
    "\n",
    "    \n",
    "    def getNumData(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqgMTMVctR4K"
   },
   "source": [
    "### Set hyperparameters & get permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "num_task = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "cuda_available = False\n",
    "if torch.cuda.is_available():\n",
    "    cuda_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5l-BVows2rQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/mnist_data/PermutedMNISTDataLoader/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:02, 3314130.95it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist_data/PermutedMNISTDataLoader/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/mnist_data/PermutedMNISTDataLoader/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 48141.24it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist_data/PermutedMNISTDataLoader/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/mnist_data/PermutedMNISTDataLoader/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 901746.75it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist_data/PermutedMNISTDataLoader/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/mnist_data/PermutedMNISTDataLoader/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 18292.85it/s]            \n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist_data/PermutedMNISTDataLoader/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "def permute_mnist():\n",
    "    train_loader = {}\n",
    "    test_loader = {}\n",
    "    \n",
    "    train_data_num = 0\n",
    "    test_data_num = 0\n",
    "    \n",
    "    for i in range(num_task):\n",
    "        shuffle_seed = np.arange(28*28)\n",
    "        np.random.shuffle(shuffle_seed)\n",
    "        \n",
    "        train_PMNIST_DataLoader = PermutedMNISTDataLoader(train=True, shuffle_seed=shuffle_seed)\n",
    "        test_PMNIST_DataLoader = PermutedMNISTDataLoader(train=False, shuffle_seed=shuffle_seed)\n",
    "        \n",
    "        train_data_num += train_PMNIST_DataLoader.getNumData()\n",
    "        test_data_num += test_PMNIST_DataLoader.getNumData()\n",
    "        \n",
    "        train_loader[i] = torch.utils.data.DataLoader(\n",
    "                train_PMNIST_DataLoader,\n",
    "                batch_size=batch_size)\n",
    "        \n",
    "        test_loader[i] = torch.utils.data.DataLoader(\n",
    "                test_PMNIST_DataLoader,\n",
    "                batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader, int(train_data_num/num_task), int(test_data_num/num_task)\n",
    "\n",
    "train_loader, test_loader, train_data_num, test_data_num = permute_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxLFhyw9tPn6"
   },
   "source": [
    "### Define Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xq7gI87s7-m"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Always start with inheriting torch.nn.Module\n",
    "        # Ancestor class of all Neural Net module\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Linear: linear transformation\n",
    "        fc1 = torch.nn.Linear(28*28, 100)\n",
    "        fc2 = torch.nn.Linear(100, 100)\n",
    "        fc3 = torch.nn.Linear(100, 100)\n",
    "  \n",
    "        \n",
    "        self.fc_module = torch.nn.Sequential(\n",
    "            fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            fc2,\n",
    "            torch.nn.ReLU(),\n",
    "            fc3\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.fc_module = self.fc_module.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_module(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Episodic Memry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEMLearning(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GEMLearning, self).__init__()\n",
    "        self.net = kwargs['net']\n",
    "        self.tasks = kwargs['tasks']\n",
    "        self.optim = kwargs['optim']\n",
    "        self.criterion = kwargs['criterion']\n",
    "        self.mem_size = kwargs['mem_size']\n",
    "        #self.num_input = kwargs['num_input']\n",
    "        self.traindata_len = kwargs['traindata_len']\n",
    "        self.testdata_len = kwargs['testdata_len']\n",
    "        self.batch_size = kwargs['batch_size']\n",
    "        \n",
    "        # Initiallize Episodic Memory\n",
    "        self.ep_mem = torch.FloatTensor(self.tasks, self.mem_size, 28*28)\n",
    "        self.ep_labels = torch.LongTensor(self.tasks, self.mem_size)\n",
    "        if cuda_available:\n",
    "            self.ep_mem = self.ep_mem.cuda()\n",
    "            self.ep_labels = self.ep_labels.cuda()\n",
    "\n",
    "        # Save each parameters' number of elements(numels)\n",
    "        self.grad_numels = []\n",
    "        for params in self.parameters():\n",
    "            self.grad_numels.append(params.data.numel())\n",
    "        print(self.grad_numels)\n",
    "        # Make matrix for gradient w.r.t. past tasks\n",
    "        self.G = torch.zeros((sum(self.grad_numels), self.tasks))\n",
    "        if cuda_available:\n",
    "            self.G = self.G.cuda()\n",
    "\n",
    "        # Make matrix for accuracy w.r.t. past tasks\n",
    "        self.R = torch.zeros((self.tasks, self.tasks))\n",
    "        if cuda_available:\n",
    "            self.R = self.R.cuda()\n",
    "\n",
    "        #msg = \"Optimizer: {}\\nCriterion: {}\\nEpisodic Memory Size: {}\\n\"%(self.optim, self.criterion, self.mem_size)\n",
    "        print(self.optim)\n",
    "        print(self.criterion)\n",
    "        print(\"Memory size: \", self.mem_size)\n",
    "        #self.log_file.write(msg)\n",
    "        \n",
    "    def train(self, data_loader, task):\n",
    "        self.cur_task = task\n",
    "        running_loss = 0.0\n",
    "        input_stack = torch.zeros((self.traindata_len, 28*28))\n",
    "        label_stack = torch.zeros((self.traindata_len))\n",
    "        if cuda_available:\n",
    "            input_stack = input_stack.cuda()\n",
    "            label_stack = label_stack.cuda()\n",
    "        \n",
    "        \n",
    "        for i, data in enumerate(data_loader):\n",
    "            #print(data)\n",
    "            x, y = data\n",
    "            #input_stack = np.vstack((input_stack, x))\n",
    "            #label_stack = np.hstack((label_stack, y))\n",
    "            input_stack[i*self.batch_size: (i+1)*self.batch_size] = x.clone()\n",
    "            label_stack[i*self.batch_size: (i+1)*self.batch_size] = y.clone()\n",
    "            if cuda_available:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                \n",
    "            \n",
    "            if self.cur_task > 0:\n",
    "                #pdb.set_trace()\n",
    "                # Compute gradient w.r.t. past tasks by using episodic memory\n",
    "                for k in range(0, self.cur_task):\n",
    "                    self.zero_grad()\n",
    "                    pred_ = self.net(self.ep_mem[k])\n",
    "                    pred_[:, : k * 10] = -10e10\n",
    "                    pred_[:, (k+1) * 10:] = -10e10\n",
    "                    \n",
    "                    pred_ = pred_[:, k*10: (k+1)*10]\n",
    "                    \n",
    "                    label_ = self.ep_labels[k]\n",
    "                    loss_ = self.criterion(pred_, label_)\n",
    "                    loss_.backward()\n",
    "        \n",
    "                    # Copy parameters into Matrix \"G\"\n",
    "                    j = 0\n",
    "                    for params in self.parameters():\n",
    "                        if params is not None:\n",
    "                            if j == 0:\n",
    "                                stpt = 0\n",
    "                            else:\n",
    "                                stpt = sum(self.grad_numels[:j])\n",
    "            \n",
    "                            endpt = sum(self.grad_numels[:j+1])\n",
    "                            self.G[stpt:endpt, k].data.copy_(params.grad.view(self.G[stpt:endpt, k].data.size()))\n",
    "                            j += 1\n",
    "                    \n",
    "            self.zero_grad()\n",
    "            \n",
    "            # Compute gradient w.r.t. current continuum\n",
    "            pred = self.net(x)#[:, self.cur_task * 10 : (self.cur_task + 1) * 10]\n",
    "            pred[:, : self.cur_task * 10] = -10e10\n",
    "            pred[:, (self.cur_task+1) * 10:] = -10e10\n",
    "                    \n",
    "            pred = pred[:, self.cur_task*10: (self.cur_task+1)*10]\n",
    "            loss = self.criterion(pred, y)\n",
    "            loss.backward()\n",
    "            running_loss += loss.detach().item()\n",
    "            \n",
    "            if i % 100 == 99:\n",
    "                msg = '[%d\\t%d] AVG. loss: %.3f\\n'% (task+1, i+1, running_loss/(i*5))\n",
    "                print(msg)\n",
    "                #self.log_file.write(msg)\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            if self.cur_task > 0:\n",
    "                grad = []\n",
    "                for param in self.net.parameters():\n",
    "                    grad.append(param.detach().view(-1))\n",
    "                \n",
    "                grad = torch.cat(grad)\n",
    "                self.G[:, self.cur_task].data.copy_(grad)\n",
    "                \n",
    "                # Solve Quadratic Problem \n",
    "                prod = torch.mm(self.G[:, self.cur_task].unsqueeze(0), self.G[:, :self.cur_task])\n",
    "                if(prod < 0).sum() != 0: # There are some violations: do projection\n",
    "                    mem_grad_np = self.G[:, :self.cur_task+1].cpu().t().double().numpy()\n",
    "                    curtask_grad_np = self.G[:, self.cur_task].unsqueeze(1).cpu().contiguous().view(-1).double().numpy()\n",
    "                    \n",
    "                    t = mem_grad_np.shape[0]\n",
    "                    P = np.dot(mem_grad_np, np.transpose(mem_grad_np))\n",
    "                    P = 0.5 * (P + np.transpose(P)) + np.eye(t) * 1e-3#eps\n",
    "                    q = np.dot(mem_grad_np, curtask_grad_np) * (-1)\n",
    "                    G = np.eye(t)\n",
    "                    h = np.zeros(t) + 0.5 #margin: hyperparameter \n",
    "                    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "                    x = np.dot(v, mem_grad_np) + curtask_grad_np\n",
    "                    #print(torch.Tensor(x).shape)\n",
    "                    #print(self.G[:, self.cur_task].shape)\n",
    "                    self.G[:, self.cur_task].copy_(torch.Tensor(x))\n",
    "                    #grad = torch.Tensor(x).view(-1).detach().clone()\n",
    "    \n",
    "                    # Copy gradients into params\n",
    "                    j = 0\n",
    "                    for params in self.parameters():\n",
    "                        if params is not None:\n",
    "                            if j == 0:\n",
    "                                stpt = 0\n",
    "                            else:\n",
    "                                stpt = sum(self.grad_numels[:j])\n",
    "        \n",
    "                            endpt = sum(self.grad_numels[:j+1])\n",
    "                            \n",
    "                            copy_object_grad = self.G[stpt:endpt, self.cur_task].contiguous().view(params.grad.data.size())\n",
    "                            params.grad.data.copy_(copy_object_grad)\n",
    "                            j += 1\n",
    "            \n",
    "            self.optim.step()\n",
    "            \n",
    "        perm = torch.randperm(self.traindata_len)\n",
    "        perm = perm[:self.mem_size]\n",
    "        self.ep_mem[self.cur_task] = input_stack[perm].detach().clone().float()\n",
    "        self.ep_labels[self.cur_task] = label_stack[perm].detach().clone()\n",
    "\n",
    "    def eval(self, data_loader, task):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        self.net.eval()\n",
    "        for i, data in enumerate(data_loader):\n",
    "            x, y = data\n",
    "            if cuda_available:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                \n",
    "            output = self.net(x)[:, task * 10: (task+1) * 10]\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "            self.R[self.cur_task][task] = 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukUD3JrixCiK"
   },
   "source": [
    "### Continual Learnig with GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0kcpGclwi6_",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78400, 100, 10000, 100, 10000, 100]\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Memory size:  100\n",
      "[1\t100] AVG. loss: 0.461\n",
      "\n",
      "[1\t200] AVG. loss: 0.225\n",
      "\n",
      "[1\t300] AVG. loss: 0.146\n",
      "\n",
      "[1\t400] AVG. loss: 0.102\n",
      "\n",
      "[1\t500] AVG. loss: 0.073\n",
      "\n",
      "[1\t600] AVG. loss: 0.051\n",
      "\n",
      "[1\t700] AVG. loss: 0.036\n",
      "\n",
      "[1\t800] AVG. loss: 0.026\n",
      "\n",
      "[1\t900] AVG. loss: 0.019\n",
      "\n",
      "tensor([[82.4700,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]], device='cuda:0')\n",
      "[2\t100] AVG. loss: 0.466\n",
      "\n",
      "[2\t200] AVG. loss: 0.232\n",
      "\n",
      "[2\t300] AVG. loss: 0.154\n",
      "\n",
      "[2\t400] AVG. loss: 0.115\n",
      "\n",
      "[2\t500] AVG. loss: 0.092\n",
      "\n",
      "[2\t600] AVG. loss: 0.077\n",
      "\n",
      "[2\t700] AVG. loss: 0.066\n",
      "\n",
      "[2\t800] AVG. loss: 0.058\n",
      "\n",
      "[2\t900] AVG. loss: 0.051\n",
      "\n",
      "tensor([[82.4700,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [59.0100,  9.7200,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]], device='cuda:0')\n",
      "[3\t100] AVG. loss: 0.465\n",
      "\n",
      "[3\t200] AVG. loss: 0.231\n",
      "\n",
      "[3\t300] AVG. loss: 0.154\n",
      "\n",
      "[3\t400] AVG. loss: 0.115\n",
      "\n",
      "[3\t500] AVG. loss: 0.092\n",
      "\n",
      "[3\t600] AVG. loss: 0.077\n",
      "\n",
      "[3\t700] AVG. loss: 0.066\n",
      "\n",
      "[3\t800] AVG. loss: 0.058\n",
      "\n",
      "[3\t900] AVG. loss: 0.051\n",
      "\n",
      "tensor([[82.4700,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [59.0100,  9.7200,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [52.6900,  9.3700, 14.0600,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]], device='cuda:0')\n",
      "[4\t100] AVG. loss: 0.465\n",
      "\n",
      "[4\t200] AVG. loss: 0.231\n",
      "\n",
      "[4\t300] AVG. loss: 0.154\n",
      "\n",
      "[4\t400] AVG. loss: 0.115\n",
      "\n",
      "[4\t500] AVG. loss: 0.092\n",
      "\n",
      "[4\t600] AVG. loss: 0.077\n",
      "\n",
      "[4\t700] AVG. loss: 0.066\n",
      "\n",
      "[4\t800] AVG. loss: 0.058\n",
      "\n",
      "[4\t900] AVG. loss: 0.051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNet()\n",
    "optim = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "memsize_list = [100, 300, 1000, 3000, 10000]\n",
    "#logfile_name = \"logfile_training_gem_%d_%d_%d_%d_%d.txt\" % (dt.year, dt.month, dt.day, dt.hour, dt.minute)\n",
    "\n",
    "for mem_size in memsize_list:\n",
    "    gem = GEMLearning(net = net,\n",
    "                          tasks = num_task,\n",
    "                          optim = optim,\n",
    "                          criterion = criterion,\n",
    "                          mem_size = mem_size,\n",
    "                          #num_input = ,\n",
    "                          traindata_len = train_data_num,\n",
    "                          testdata_len = test_data_num,\n",
    "                          batch_size = batch_size)\n",
    "    \n",
    "    for i in range(num_task):\n",
    "        gem.train(train_loader[i], i)\n",
    "        \n",
    "        for j in range(i+1):\n",
    "            gem.eval(test_loader[j], j)\n",
    "            \n",
    "        print(gem.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_GEM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
